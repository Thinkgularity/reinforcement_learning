# Отчет 2. Исследование метода Q-learning в среде Frozen Lake 

## 1. Сравнение алгоритмов V и Q learning (2 балла)
Для алгоритма `V learning` на поле (4х4) при `gamma=0.9` сходимость (mean reward > 0.85) достигается в среднем за 29 итерации (от 17 до 66). 
Графики зависимости reward от количества итераций приведены ниже. 

<img src="\images\9g_v_iteration.jpg"/>

Для алгоритма `Q learning` на поле (4х4) при `gamma=0.9` сходимость (mean reward > 0.9) достигается в среднем за 31 итерации (от 20 до 52). 
Графики зависимости reward от количества итераций приведены ниже. 

<img src="\images\9g_q_iteration.jpg"/>

**Вывод:** Делать вывод о том какой алгоритм лучше необходимо в зависимости от поставленной перед нами задачи, поскольку у алгоритма действий  `Q learning` сходимость достигается в среднем за 31 итерацию, а средняя награда получается 0.9 в отличии от алгоритма ценностей, который быстрее достигает сходимости, но с чуть меньшей наградой. То есть если нам важна скорость обучения - лучшим вариантом будет алгоритм ценностей `V learning`, в случае если же нам более важно качество алгоритма, я бы предпочел - алгоритм действий  `Q learning`.


## 2. Влияние гиперпараметра `GAMMA` на скорость сходимости . (2 балла)

**Проверим скорость сходимости в случае уменьшения гиперпараметра с

Для алгоритма `Q learning` на поле (4х4) при `gamma=0.8` сходимость (mean reward > 0.9) достигается в среднем за 114 итераций (от 15 до 419). Графики зависимости reward от количества итераций приведены ниже. 

<img src="\images\8g_q_iteration.jpg"/>

Для алгоритма `V learning` на поле (4х4) при `gamma=0.8` сходимость (mean reward > 0.85) достигается в среднем за 995 итераций (от 26 до 4046). Графики зависимости reward от количества итераций приведены ниже.

<img src="\images\8g_v_iteration.jpg"/>

**Проверим скорость сходимости модели в случае уведличения гиперпараметра `GAMMA`:**

Для алгоритма `Q learning` на поле (4х4) при `gamma=1.0` сходимость (mean reward > 0.9) достигается в среднем за 22 итераций (от 14 до 35). Графики зависимости reward от количества итераций приведены ниже. 

<img src="\images\1.0_q_iteration.jpg"/>

Для алгоритма `V learning` на поле (4х4) при `gamma=0.8` сходимость (mean reward > 0.85) достигается в среднем за 20 итераций (от 16 до 30). Графики зависимости reward от количества итераций приведены ниже.

<img src="\images\1.0_v_iteration.jpg"/>

**Вывод:** Увеличение гиперпараметра `GAMMA` приводит к уменьшению количества итераций.  

Уменьшение гиперпараметра `GAMMA` приводит к увеличению времени сходимости модели. Это связано с тем, что Гамма отвечает за то насколько наш агент будет жадным, уменьшая гамма -мы делаем агента более жадным, в связи с чем ему требуется времени и наоборот.  

## 3. Сравнение алгоритмов V и Q learning на поле большего размера (3 балла)

Для алгоритма `Q learning` на поле (8х8) при `gamma=0.9` сходимость (mean reward > 0.9) достигается в среднем за 184 итераций (от 46 до 354). Графики зависимости reward от количества итераций приведены ниже.

<img src="\images\8_8_q_iteration.png"/>

Для алгоритма `V learning` на поле (8х8) при `gamma=0.9` сходимость (mean reward > 0.85) достигается в среднем за 1 410 итераций (от 252 до 3 125). Графики зависимости reward от количества итераций приведены ниже.

<img src="\images\8_8_v_iteration.jpg"/>

**Вывод:** В итоге мы наблюдаем существенное увеличение количетсва итераций, и, соответственно времени, требуемого на обучение модели. Стоит отметить, что при алгоритме действия сходимость достигается в среднем за 184 итерации, когда при алгоритме ценности в среднем за 1410 итерации.