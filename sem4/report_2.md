# Отчет 2. Исследование метода Q-learning в среде Frozen Lake 

## 1. Влияние гиперпараметра альфа на среднее количество шагов обучения (>=5 повторений) (2 балла)

# Alpha=0.2

Для алгоритма `Q learning` при `gamma=0.9` и `alpha=0.2` сходимость (mean reward > 0.9) достигается в среднем за 7 986 итерации (от 2 291 до 16 825). 
Графики зависимости reward от количества итераций приведены ниже. 

<img src="\images\alpha02.jpg"/>

# Alpha=0.0001

Для алгоритма `Q learning` при `gamma=0.9` и `alpha=0.0001` сходимость достигается в среднем за 52 273 итерации (от 47 579 до 56 967). На графике представлены только две итерации, поскольку в остальных случаях модель просто "застревает в локальных минимумах" и количество итераций становится слишком огромным, что напрямую коррелирует со временем обучения модели. 
Графики зависимости reward от количества итераций приведены ниже. 

<img src="\images\alpha0.0001.jpg"/>

# Alpha=0.5

Для алгоритма `Q learning` при `gamma=0.9` и `alpha=0.5` сходимость (mean reward > 0.9) достигается в среднем за 5322 итерации (от 2777 до 12 262). 
Графики зависимости reward от количества итераций приведены ниже.

<img src="\images\alpha0.5.jpg"/>

# Alpha=1.0 

Для алгоритма `Q learning` при `gamma=0.9` и `alpha=0.5` сходимость (mean reward > 0.85) достигается в среднем за 7 623,5 итерации (от 3 980 до 12 893). 
Графики зависимости reward от количества итераций приведены ниже.

<img src="\images\alpha1.0.jpg"/>

**Вывод:** Как мы можем наблюдать, гиперпараметр `alpha` сильно влияет на скорость обучения модели. Не трудно заметить, что если `alpha` стремится к нулю, то начинаются серьезные проблемы в обучении модели: обучение сильно замедляется или становится невозможным вследствии локальных минимумов. На мой взгляд, из проведенных эксперементов - самым оптимальным для алгоритма `Q learning` получился результат с установленным гиперпараметром при `gamma=0.9` и `alpha=0.5`. В этом случае у нас наблюдается баланс качества и скорости обучения модели. В случае с `alpha=0.2` у нас просадка в скорости, а в случае с `alpha=1.0` - начинает появляется просадка в качестве.


## 3.  Влияние гиперпараметров (Deep Q learning в среде Pong) на среднее количество шагов обучения. (2 балла)

**Проверим скорость сходимости в случае уменьшения гиперпараметра с

Для алгоритма `Q learning` на поле (4х4) при `gamma=0.8` сходимость (mean reward > 0.9) достигается в среднем за 114 итераций (от 15 до 419). Графики зависимости reward от количества итераций приведены ниже. 

<img src="\images\8g_q_iteration.jpg"/>

Для алгоритма `V learning` на поле (4х4) при `gamma=0.8` сходимость (mean reward > 0.85) достигается в среднем за 995 итераций (от 26 до 4046). Графики зависимости reward от количества итераций приведены ниже.

<img src="\images\8g_v_iteration.jpg"/>

**Проверим скорость сходимости модели в случае уведличения гиперпараметра `GAMMA`:**

Для алгоритма `Q learning` на поле (4х4) при `gamma=1.0` сходимость (mean reward > 0.9) достигается в среднем за 22 итераций (от 14 до 35). Графики зависимости reward от количества итераций приведены ниже. 

<img src="\images\1.0_q_iteration.jpg"/>

Для алгоритма `V learning` на поле (4х4) при `gamma=0.8` сходимость (mean reward > 0.85) достигается в среднем за 20 итераций (от 16 до 30). Графики зависимости reward от количества итераций приведены ниже.

<img src="\images\1.0_v_iteration.jpg"/>

**Вывод:** Увеличение гиперпараметра `GAMMA` приводит к уменьшению количества итераций.  

Уменьшение гиперпараметра `GAMMA` приводит к увеличению времени сходимости модели. Это связано с тем, что Гамма отвечает за то насколько наш агент будет жадным, уменьшая гамма -мы делаем агента более жадным, в связи с чем ему требуется времени и наоборот.  

## 3. Сравнение алгоритмов V и Q learning на поле большего размера (3 балла)

Для алгоритма `Q learning` на поле (8х8) при `gamma=0.9` сходимость (mean reward > 0.9) достигается в среднем за 184 итераций (от 46 до 354). Графики зависимости reward от количества итераций приведены ниже.

<img src="\images\8_8_q_iteration.png"/>

Для алгоритма `V learning` на поле (8х8) при `gamma=0.9` сходимость (mean reward > 0.85) достигается в среднем за 1 410 итераций (от 252 до 3 125). Графики зависимости reward от количества итераций приведены ниже.

<img src="\images\8_8_v_iteration.jpg"/>

**Вывод:** В итоге мы наблюдаем существенное увеличение количетсва итераций, и, соответственно времени, требуемого на обучение модели. Стоит отметить, что при алгоритме действия сходимость достигается в среднем за 184 итерации, когда при алгоритме ценности в среднем за 1410 итерации.