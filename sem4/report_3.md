# Отчет 2. Исследование метода Q-learning в среде Frozen Lake 

## 1. Влияние гиперпараметра альфа на среднее количество шагов обучения (>=5 повторений) (2 балла)

# Alpha=0.2

Для алгоритма `Q learning` при `gamma=0.9` и `alpha=0.2` сходимость (mean reward > 0.9) достигается в среднем за 7 986 итерации (от 2 291 до 16 825). 
Графики зависимости reward от количества итераций приведены ниже. 

<img src="\images\alpha02.jpg"/>

# Alpha=0.0001

Для алгоритма `Q learning` при `gamma=0.9` и `alpha=0.0001` сходимость достигается в среднем за 52 273 итерации (от 47 579 до 56 967). На графике представлены только две итерации, поскольку в остальных случаях модель просто "застревает в локальных минимумах" и количество итераций становится слишком огромным, что напрямую коррелирует со временем обучения модели. 
Графики зависимости reward от количества итераций приведены ниже. 

<img src="\images\alpha0.0001.jpg"/>

# Alpha=0.5

Для алгоритма `Q learning` при `gamma=0.9` и `alpha=0.5` сходимость (mean reward > 0.9) достигается в среднем за 5322 итерации (от 2777 до 12 262). 
Графики зависимости reward от количества итераций приведены ниже.

<img src="\images\alpha0.5.jpg"/>

# Alpha=1.0 

Для алгоритма `Q learning` при `gamma=0.9` и `alpha=1.0` сходимость (mean reward > 0.85) достигается в среднем за 7 623,5 итерации (от 3 980 до 12 893). 
Графики зависимости reward от количества итераций приведены ниже.

<img src="\images\alpha1.0.jpg"/>

**Вывод:** Как мы можем наблюдать, гиперпараметр `alpha` сильно влияет на скорость обучения модели. Не трудно заметить, что если `alpha` стремится к нулю, то начинаются серьезные проблемы в обучении модели: обучение сильно замедляется или становится невозможным вследствии локальных минимумов. На мой взгляд, из проведенных эксперементов - самым оптимальным для алгоритма `Q learning` получился результат с установленным гиперпараметром при `gamma=0.9` и `alpha=0.5`. В этом случае у нас наблюдается баланс качества и скорости обучения модели. В случае с `alpha=0.2` у нас просадка в скорости, а в случае с `alpha=1.0` - начинает появляется просадка в качестве.

## Изучите алгоритм глубокого обучения (Deep Q learning) в среде Pong: Chapter06/02_dqn_pong.py Обучите сеть с гиперпараметрами по умолчанию и запишите видео Chapter06/03_dqn_play.py (2 балла).

Мы обучили алгоритм и получили следующий результат: Total reward: -9.00. На видео видно, как модель практически одну минуту справляется с игрой. Данный результат не является лучшим из возможных, но наш агент хорошо справляется с поставленной перед ним задачей. 


## 3.  Влияние гиперпараметров (Deep Q learning в среде Pong) на среднее количество шагов обучения. (2 балла)

Какие гиперпараметры будем изменять? Во-первых, умнешьшим `batch_size` с 32 до 16 объектов. Во-вторых, немного увеличим `learning_rate` c 0.0001 до 0.01 и посмотрим как поведет себя агент в таких условиях. 

Результат: Мы получили только отрицательный результат, меняя данные гиперпараметры, в итоге, агент модель застревает в локальном минимуме, мы не можем получить награду больше, чем -20.760, при условии, что модель обучалась один час. 

Если мы вернем гиперпараметры по умолчанию и попробуем изменить гамму в сторону увеличения - то увидем, что  увеличение гаммы имеет тот же результат, что и попытка выше - локальный минимум, а вот уменьшение гаммы приводит к более лучшему результату обучения модели, но от этого страдает скорость обучения. Видео с уменьшенной гаммой представлено по пути: /sem4/video/гамма меньше.mp4